<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>AirSim的api接口</title>
    <link href="/2020/09/29/AirSim-API/"/>
    <url>/2020/09/29/AirSim-API/</url>
    
    <content type="html"><![CDATA[<p>原链接：<br><a href="https://microsoft.github.io/AirSim/apis/#common-apis">https://microsoft.github.io/AirSim/apis/#common-apis</a><br>(下文为原文谷歌翻译)</p><h1 id="常用API"><a href="#常用API" class="headerlink" title="常用API"></a>常用API</h1><ul><li><code>reset</code>:这将车辆重置为其原始启动状态。请注意，必须在调用重置后再次调用<code>enableApiControl</code>和<code>armDisarm</code>。</li><li><code>ConfirmConnection</code>：每1秒检查一次连接状态，并在控制台中报告该状态，以便用户可以查看连接进度。</li><li><code>enableApiControl</code>：出于安全原因，默认情况下不会启用对自动驾驶汽车的API控制，并且操作员具有完全控制权（通常通过RC或模拟器中的操纵杆）。客户端必须进行此调用才能通过API请求控制。车辆的操作员可能禁止了API控制，这意味着<code>enableApiControl</code>不起作用。可以通过检查<code>isApiControlEnabled</code>。</li><li><code>isApiControlEnabled</code>：如果建立了API控件，则返回true。如果为false（默认设置），则将忽略API调用。成功调用后<code>enableApiControl</code>，<code>isApiControlEnabled</code>应当返回true。</li><li><code>ping</code>：如果建立了连接，则此调用将返回true，否则它将被阻止直到超时。</li><li><code>simPrintLogMessage</code>：在模拟器的窗口中打印指定的消息。如果还提供了message_param，则将其打印在消息旁边，在这种情况下，如果使用相同的消息值但又使用不同的message_param调用此API，则前一行将用新行覆盖（而不是API在显示时创建新行）。例如，<code>simPrintLogMessage(&quot;Iteration: &quot;, to_string(i))</code>当使用不同的i值调用API时，将不断更新显示的同一行。严重性参数的有效值为0到3（含0和3），分别对应于不同的颜色。</li><li><code>simGetObjectPose</code>，<code>simSetObjectPose</code>：获取并设置虚幻环境中指定对象的姿势。这里的对象在虚幻的术语中指的是“演员”。它们通过标签和名称进行搜索。请注意，UE编辑器中显示的名称在每次运行时都会自动生成，并且不是永久的。因此，如果要按名称引用actor，则必须在UE Editor中更改其自动生成的名称。另外，您可以向演员添加标签，方法是在虚幻编辑器中单击该演员，然后转到“标签”属性，单击“ +”号并添加一些字符串值。如果多个演员具有相同的标签，则返回第一个匹配项。如果找不到匹配项，则返回NaN pose。返回的姿势在NED坐标中以SI单位表示，其原点在“播放器开始”处。对于<code>simSetObjectPose</code>，则指定的actor必须将Mobility设置为Movable，否则您将获得未定义的行为。该<code>simSetObjectPose</code>具有的参数<code>teleport</code>，这意味着对象通过其他物体移动在它的方式，如果此举成功，就返回true</li></ul><h1 id="图像-计算机视觉API"><a href="#图像-计算机视觉API" class="headerlink" title="图像/计算机视觉API"></a>图像/计算机视觉API</h1><p>AirSim提供了全面的图像API，可从多个摄像机检索同步图像以及地面真实情况，包括<code>深度</code>，<code>视差</code>，<code>表面法线</code>和<code>视觉</code>。您可以在settings.json中设置分辨率，FOV，运动模糊等参数。还有用于检测碰撞状态的API。另请参见完整代码，该代码生成特定数量的立体图像和地面真相深度，并根据相机计划进行标准化，计算视差图像并将其保存为pfm格式。</p><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;SettingsVersion&quot;</span>: <span class="hljs-number">1.2</span>,  <span class="hljs-attr">&quot;CameraDefaults&quot;</span>: &#123;      <span class="hljs-attr">&quot;CaptureSettings&quot;</span>: [        &#123;          <span class="hljs-attr">&quot;ImageType&quot;</span>: <span class="hljs-number">0</span>,          <span class="hljs-attr">&quot;Width&quot;</span>: <span class="hljs-number">256</span>,          <span class="hljs-attr">&quot;Height&quot;</span>: <span class="hljs-number">144</span>,          <span class="hljs-attr">&quot;FOV_Degrees&quot;</span>: <span class="hljs-number">90</span>,          <span class="hljs-attr">&quot;AutoExposureSpeed&quot;</span>: <span class="hljs-number">100</span>,          <span class="hljs-attr">&quot;MotionBlurAmount&quot;</span>: <span class="hljs-number">0</span>        &#125;    ]  &#125;,  <span class="hljs-attr">&quot;SimMode&quot;</span>: <span class="hljs-string">&quot;ComputerVision&quot;</span>&#125;</code></pre><h1 id="暂停和继续API"><a href="#暂停和继续API" class="headerlink" title="暂停和继续API"></a>暂停和继续API</h1><p>AirSim允许通过<code>pause(is_paused)API</code>暂停并继续仿真。暂停模拟通话<code>pause(True)</code>并继续模拟通话<code>pause(False)</code>。您可能会遇到一些情况，尤其是在使用强化学习时，要在指定的时间量内运行模拟，然后自动暂停。暂停模拟后，您可能需要执行一些昂贵的计算，发送新命令，然后在指定的时间内再次运行模拟。这可以通过API实现continueForTime(seconds)。此API在指定的秒数内运行模拟，然后暂停模拟。有关用法示例，请参见pause_continue_car.py和pause_continue_drone.py。</p><h1 id="碰撞API"><a href="#碰撞API" class="headerlink" title="碰撞API"></a>碰撞API</h1><p>可以使用<code>simGetCollisionInfo</code>API获取冲突信息。该调用返回一个结构，该结构不仅具有是否发生碰撞的信息，而且还具有碰撞位置，表面法线和穿透深度等信息。</p><h1 id="时段API"><a href="#时段API" class="headerlink" title="时段API"></a>时段API</h1><p>AirSim假设类的存在天空球EngineSky/BP_Sky_Sphere在你的环境中ADirectionalLight演员。默认情况下，场景中太阳的位置不会随时间移动。您可以使用设置设置AirSim用于计算场景中太阳位置的纬度，经度，日期和时间。</p><p>您还可以使用以下API调用根据给定的日期时间设置太阳的位置：</p><pre><code class="hljs python">simSetTimeOfDay(self, is_enabled, start_datetime = <span class="hljs-string">&quot;&quot;</span>, is_start_datetime_dst = <span class="hljs-literal">False</span>, celestial_clock_speed = <span class="hljs-number">1</span>, update_interval_secs = <span class="hljs-number">60</span>, move_sun = <span class="hljs-literal">True</span>)</code></pre><p>该<code>is_enabled</code>参数必须是<code>True</code>启用一天中的时间效果。如果是，<code>False</code>则在环境中将太阳位置重置为其原始位置。</p><h1 id="天气API"><a href="#天气API" class="headerlink" title="天气API"></a>天气API</h1><p>默认情况下，所有天气影响都是禁用的。要启用天气效果，请先call：</p><blockquote><p>simEnableWeather(True)</p></blockquote><p>各种天气的影响可通过使用启用simSetWeatherParameter方法，该方法采用WeatherParameter，例如，</p><blockquote><p>client.simSetWeatherParameter(airsim.WeatherParameter.Rain, 0.25);</p></blockquote><p>第二个参数值是从0到1。第一个参数提供以下选项：</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WeatherParameter</span>:</span>    Rain = <span class="hljs-number">0</span>    Roadwetness = <span class="hljs-number">1</span>    Snow = <span class="hljs-number">2</span>    RoadSnow = <span class="hljs-number">3</span>    MapleLeaf = <span class="hljs-number">4</span>    RoadLeaf = <span class="hljs-number">5</span>    Dust = <span class="hljs-number">6</span>    Fog = <span class="hljs-number">7</span></code></pre><p>请注意<code>Roadwetness</code>，<code>RoadSnow</code>和<code>RoadLeaf</code>效果需要向场景添加材质。</p><h1 id="多无人机"><a href="#多无人机" class="headerlink" title="多无人机"></a>多无人机</h1><pre><code class="hljs json">&#123;    <span class="hljs-attr">&quot;SettingsVersion&quot;</span>: <span class="hljs-number">1.2</span>,    <span class="hljs-attr">&quot;SimMode&quot;</span>: <span class="hljs-string">&quot;Multirotor&quot;</span>,    <span class="hljs-attr">&quot;Vehicles&quot;</span>: &#123;        <span class="hljs-attr">&quot;Drone1&quot;</span>: &#123;          <span class="hljs-attr">&quot;VehicleType&quot;</span>: <span class="hljs-string">&quot;SimpleFlight&quot;</span>,          <span class="hljs-attr">&quot;X&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-attr">&quot;Y&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-attr">&quot;Z&quot;</span>: <span class="hljs-number">-2</span>,      <span class="hljs-attr">&quot;Yaw&quot;</span>: <span class="hljs-number">-180</span>        &#125;,        <span class="hljs-attr">&quot;Drone2&quot;</span>: &#123;          <span class="hljs-attr">&quot;VehicleType&quot;</span>: <span class="hljs-string">&quot;SimpleFlight&quot;</span>,          <span class="hljs-attr">&quot;X&quot;</span>: <span class="hljs-number">8</span>, <span class="hljs-attr">&quot;Y&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-attr">&quot;Z&quot;</span>: <span class="hljs-number">-2</span>        &#125;    &#125;&#125;</code></pre><h1 id="图片API"><a href="#图片API" class="headerlink" title="图片API"></a>图片API</h1><h2 id="获取单个图像"><a href="#获取单个图像" class="headerlink" title="获取单个图像"></a>获取单个图像</h2><pre><code class="hljs python"><span class="hljs-keyword">import</span> airsim <span class="hljs-comment">#pip install airsim</span>client = airsim.MultirotorClient()png_image = client.simGetImage(<span class="hljs-string">&quot;0&quot;</span>, airsim.ImageType.Scene)</code></pre><h2 id="更加强大的"><a href="#更加强大的" class="headerlink" title="更加强大的"></a>更加强大的</h2><p>例如，您可以在一个API调用中从左摄像机获取左摄像机视图，右摄像机视图和深度图像。</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> airsim <span class="hljs-comment">#pip install airsim</span>client = airsim.MultirotorClient()responses = client.simGetImages([    <span class="hljs-comment"># png format</span>    airsim.ImageRequest(<span class="hljs-number">0</span>, airsim.ImageType.Scene),     <span class="hljs-comment"># uncompressed RGB array bytes</span>    airsim.ImageRequest(<span class="hljs-number">1</span>, airsim.ImageType.Scene, <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>),    <span class="hljs-comment"># floating point uncompressed image</span>    airsim.ImageRequest(<span class="hljs-number">1</span>, airsim.ImageType.DepthPlanner, <span class="hljs-literal">True</span>)]) <span class="hljs-comment"># do something with response which contains image data, pose, timestamp etc</span></code></pre><h2 id="通过NumPy使用AirSim图像"><a href="#通过NumPy使用AirSim图像" class="headerlink" title="通过NumPy使用AirSim图像"></a>通过NumPy使用AirSim图像</h2><p>如果计划使用numpy进行图像处理，则应获取未压缩的RGB图像，然后将其转换为numpy，如下所示：</p><pre><code class="hljs python">responses = client.simGetImages([airsim.ImageRequest(<span class="hljs-string">&quot;0&quot;</span>, airsim.ImageType.Scene, <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>)])response = responses[<span class="hljs-number">0</span>]<span class="hljs-comment"># get numpy array</span>img1d = np.fromstring(response.image_data_uint8, dtype=np.uint8) <span class="hljs-comment"># reshape array to 4 channel image array H X W X 4 (这里怀疑采集有误，可能是针对的是RGB-D图像)</span>img_rgb = img1d.reshape(response.height, response.width, <span class="hljs-number">3</span>)<span class="hljs-comment"># original image is fliped vertically</span>img_rgb = np.flipud(img_rgb)<span class="hljs-comment"># write to png </span>airsim.write_png(os.path.normpath(filename + <span class="hljs-string">&#x27;.png&#x27;</span>), img_rgb)</code></pre><p><code>其他提示</code>：</p><blockquote><p><code>APIsimGetImage</code>返回<code>binary string literal</code>，这意味着您可以简单地将其转储为二进制文件以创建.png文件。但是，如果您希望以方便的功能之外的其他方式处理它<code>airsim.string_to_uint8_array</code>。这会将二进制字符串文字转换为NumPy uint8数组。<br><code>APIsimGetImages</code>可以在一次调用中接受来自任何摄像机的多种图像类型的请求。您可以指定图像是png压缩，RGB未压缩还是float数组。对于png压缩图像，您得到<code>binary string literal</code>。对于float数组，您将获得float64的Python列表。您可以使用以下方法将此float数组转换为NumPy 2D数组</p></blockquote><pre><code class="hljs python">airsim.list_to_2d_float_array(response.image_data_float, response.width, response.height)</code></pre><blockquote><p>您也可以使用<code>airsim.write_pfm()</code>函数将浮点数组保存为.pfm文件（便携式浮点图格式）。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>仿真</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AirSim</tag>
      
      <tag>UAVs</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>目标检测20年【tracking】</title>
    <link href="/2020/09/27/%E8%A7%86%E8%A7%89%E8%B7%9F%E8%B8%AA20%E5%B9%B4%E3%80%90tracking%E3%80%91/"/>
    <url>/2020/09/27/%E8%A7%86%E8%A7%89%E8%B7%9F%E8%B8%AA20%E5%B9%B4%E3%80%90tracking%E3%80%91/</url>
    
    <content type="html"><![CDATA[<h2 id="检测算法指南"><a href="#检测算法指南" class="headerlink" title="检测算法指南"></a>检测算法指南</h2><p>可能只包含了我感兴趣的内容，具体请参见arxiv:1905.05055</p><blockquote><p>Object Detection in 20 Years: A Survey</p></blockquote><h3 id="传统目标检测（2014年前）："><a href="#传统目标检测（2014年前）：" class="headerlink" title="传统目标检测（2014年前）："></a>传统目标检测（2014年前）：</h3><ul><li><p>Viola Jones Detectors<br>Integral image, Feature selection, Detection cascades</p></li><li><p>HOG Detector<br>scale-invariant feature transform</p></li><li><p>Deformable Part-based Model (DPM)<br>mixture models, hard negative mining, bounding box regression, etc.</p></li></ul><h3 id="基于深度学习的目标检测："><a href="#基于深度学习的目标检测：" class="headerlink" title="基于深度学习的目标检测："></a>基于深度学习的目标检测：</h3><h4 id="Two-stage-Detectors"><a href="#Two-stage-Detectors" class="headerlink" title="Two-stage Detectors"></a>Two-stage Detectors</h4><ul><li>RCNN<br>加入CNN</li><li>SPPNet<br>生成固定表达的向量</li><li>Fast RCNN<br>框回归集成</li><li>Faster RCNN<br>Region Proposal Network (RPN)</li><li>Feature Pyramid Networks</li></ul><h4 id="One-stage-Detectors"><a href="#One-stage-Detectors" class="headerlink" title="One-stage Detectors"></a>One-stage Detectors</h4><ul><li>YOLO, v2, v3(R. Joseph)<br>边界框定位精度问题</li><li>Single Shot MultiBox Detector (SSD)<br>加入了不同尺度层的信息</li><li>RetinaNet<br>focal loss<blockquote><p>They claimed that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause.</p></blockquote></li></ul><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><ul><li>Pascal VOC</li><li>ILSVRC， 有更多的类别<code>200类</code></li><li>MS-COCO, 包含更多的目标实例<code>80类</code></li><li>Open Images, 更多的检测，又增加了物体相对关系的检测<code>600类</code></li></ul><h2 id="测度"><a href="#测度" class="headerlink" title="测度"></a>测度</h2><ul><li>行人识别，false positives perimage(FPPI)</li><li>目标识别，Average Precision (AP)；目标定位准确，IoU(&gt;0.5)</li><li>粗定位结果IoU(&gt;0.5)，完美定位IoU(&gt;0.95)</li></ul><h2 id="Multi-reference-resolution-detection-after-2015"><a href="#Multi-reference-resolution-detection-after-2015" class="headerlink" title="Multi-reference/-resolution detection (after 2015)"></a>Multi-reference/-resolution detection (after 2015)</h2><blockquote><p>Its main idea is to pre-define a set of reference boxes (a.k.a. anchor boxes) with different sizes and aspect-ratios at different locations of an image, and then predict the detection box based on these references.</p></blockquote><p>典型的损失包含，1）类别识别的交叉熵损失；2）L1 or L2 回归损失（边界框定位）</p><h2 id="BB的演进"><a href="#BB的演进" class="headerlink" title="BB的演进"></a>BB的演进</h2><p>2013年后，采用的框盒预测是从特征到BB。为了更加鲁棒的预测，常采用smooth-L1 function或者root-square function。</p><h2 id="考虑上下文环境"><a href="#考虑上下文环境" class="headerlink" title="考虑上下文环境"></a>考虑上下文环境</h2><blockquote><p>Recent deep learning based detectors can also be improved with local context by simply enlarging the networks’ receptive field or the size of object proposals.</p></blockquote><ul><li><p>对全局的背景信息可以采用增大足够大的感受野 或者 全局池化操作</p></li><li><p>认为全局上下文信息是一种序列信息，可以采用RNN来学习到</p></li><li><p>现在，一种趋势是获得物体间的相互依赖关系，用以提升准确率！！！</p></li></ul><h2 id="Non-maximum-suppression-NMS"><a href="#Non-maximum-suppression-NMS" class="headerlink" title="Non-maximum suppression (NMS)"></a>Non-maximum suppression (NMS)</h2><blockquote><p>the non-maximum suppression is herein used as a post-processing step to remove the replicated bounding boxes and obtain the final detection result.</p></blockquote><h2 id="Hard-Negative-Mining"><a href="#Hard-Negative-Mining" class="headerlink" title="Hard Negative Mining"></a>Hard Negative Mining</h2><blockquote><p>Hard negative mining (HNM) aims to deal with the problem of imbalanced data during training.</p></blockquote><h3 id="一些技术"><a href="#一些技术" class="headerlink" title="一些技术"></a>一些技术</h3><ul><li>Bootstrap，逐步添加数据进去 (first)</li><li>weight-balancing，但是不能完全解决</li><li>新的损失函数，改造了标准的交叉熵</li></ul><h2 id="feature-map-shared-computation"><a href="#feature-map-shared-computation" class="headerlink" title="feature map shared computation"></a>feature map shared computation</h2><p>典型的参见RNN系列</p><h2 id="级联检测"><a href="#级联检测" class="headerlink" title="级联检测"></a>级联检测</h2><p>难度分级，如今应用在</p><ul><li>small objects in large scenes</li></ul><p>加速加成</p><ul><li>提高检测困难例子</li><li>集成上下文信息</li><li>提高定位准确率</li></ul><h2 id="Network-Pruning-and-Quantification"><a href="#Network-Pruning-and-Quantification" class="headerlink" title="Network Pruning and Quantification"></a>Network Pruning and Quantification</h2><p>除此以外，Network Distillation（教师网络，学生网络）</p><h2 id="轻量化网络结构"><a href="#轻量化网络结构" class="headerlink" title="轻量化网络结构"></a>轻量化网络结构</h2><ul><li>Factorizing Convolutions（叠加小卷积）</li><li>Group Convolution - 分部分进行卷积，最后组合在一起</li><li>Depth-wise Separable Convolution - 应用在目标检测和细粒度分类中</li><li>Bottle-neck Design - 更为有表达性的信息编码</li><li>NAS</li></ul><h2 id="数值加速方法"><a href="#数值加速方法" class="headerlink" title="数值加速方法"></a>数值加速方法</h2><ul><li>Integral Image<blockquote><p>The integral HOG map has been used in pedestrian detection and has achieved dozens of times’ acceleration without losing any accuracy</p></blockquote></li><li>Frequency Domain<blockquote><p>FFT加速</p></blockquote></li><li>Vector Quantization (VQ)</li><li>Reduced Rank Approximation</li></ul><p>(SENet采用特征加权的融合方法)</p><h2 id="特征融合"><a href="#特征融合" class="headerlink" title="特征融合"></a>特征融合</h2><ul><li>bottom-up fusion, (b) top-down fusion, (c) element-wise sum, (d) element-wise product, and (e) concatenation.</li><li>fractional strided convolution for up-sampling</li></ul><p>关于空洞卷积的一些缘由：</p><blockquote><p>As we mentioned before, the lower the feature resolution is, the harder will be to detect small objects. The most straight forward way to increase the feature resolution is to remove pooling layer or to reduce the convolution downsampling rate. But this will cause a new problem, the receptive field will become too small due to the decreasingof output stride. In other words, this will narrow a detector’s ”sight” and may result in the miss detection of some large objects.<br>A piratical method to increase both of the receptive field and feature resolution at the same time is to introduce dilated convolution (a.k.a. atrous convolution, or convolution with holes). Dilated convolution is originally proposed in semantic segmentation tasks. Its main idea is to expand the convolution filter and use sparse parameters. For example, a 3x3 filter with a dilation rate of 2 will have the same receptive field as a 5x5 kernel but only have 9 parameters. Dilated convolution has now been widely used in object detection, and proves to be effective for improved accuracy without any additional parameters and computational cost.</p></blockquote><h2 id="滑动窗口之外"><a href="#滑动窗口之外" class="headerlink" title="滑动窗口之外"></a>滑动窗口之外</h2><ul><li>Sub-region search，以路径规划理解</li><li>key points localization<blockquote><p>One recent implementation of this idea is to predict a heat-map for the corners.<br>The advantage of this approach is that it can be implemented under a semantic segmentation framework, and there is no need to design multi-scale anchor boxes.</p></blockquote></li></ul><h2 id="提高定位准确率"><a href="#提高定位准确率" class="headerlink" title="提高定位准确率"></a>提高定位准确率</h2><ul><li>BB微调</li><li>设计新的损失</li></ul><h2 id="在分割任务中学习"><a href="#在分割任务中学习" class="headerlink" title="在分割任务中学习"></a>在分割任务中学习</h2><blockquote><p>Recent researches suggest object detection can be improved by learning with semantic segmentation.</p></blockquote><ul><li>Edges and boundaries are the basic elements that constitute human visual cognition.</li><li>The ground-truth bounding box of an object is determined by its well-defined boundary, learning with segmentation would be helpful for accurate object localization.</li><li>Integrating the context of semantic segmentation will be helpful for object detection.</li></ul><h3 id="怎么样提升？"><a href="#怎么样提升？" class="headerlink" title="怎么样提升？"></a>怎么样提升？</h3><ul><li>The simplest way is to think of the segmentation network as a fixed feature extractor and to integrate it into a detection framework as additional features</li><li>Another way is to introduce an additional segmentation branch on top of the original detection framework and to train this model with multi-task loss functions (segmentation loss + detection loss). In most cases, the segmentation brunch will be removed at the inference stage. The advantage is the detection speed will not be affected, but the disadvantage is that the training requires pixel-level image annotations. To this end, some researchers have followed the idea of “weakly supervised learning”: instead of training based on pixel-wise annotation masks, they simply train the segmentation brunch based on the bounding-box level annotations.</li></ul><h2 id="对旋转和放缩鲁棒"><a href="#对旋转和放缩鲁棒" class="headerlink" title="对旋转和放缩鲁棒"></a>对旋转和放缩鲁棒</h2><h3 id="旋转鲁棒"><a href="#旋转鲁棒" class="headerlink" title="旋转鲁棒"></a>旋转鲁棒</h3><ul><li>旋转不变的损失函数</li><li>旋转校正 - Spatial Transformer Networks (STN) 用作文本检测任务的旋转校正</li><li>旋转感兴趣区域的池化层<blockquote><p>In a two-stage detector, feature pooling aims to extract a fixed length feature representation for an object proposal with any location and size by first dividing the proposal evenly into a set of grids, and then concatenating the grid features. As the grid meshing is performed in Cartesian coordinates, the features are not invariance to rotation transform. A recent improvement is to mesh the grids in polar coordinates so that the features could be robust to the rotation changes.</p></blockquote><h3 id="放缩鲁棒"><a href="#放缩鲁棒" class="headerlink" title="放缩鲁棒"></a>放缩鲁棒</h3></li><li>简而言之就是因为图片上物体的检测难以覆盖全面，总会存在物体过小、过大检测不出的问题</li><li>一是采用局部区域放缩</li><li>“adaptive zoom-in” techniques</li><li>Another recent improvement is learning to predict the scale distribution of objects in an image, and then adaptively re-scaling the image according to the distribution.</li></ul><h2 id="对抗自训练"><a href="#对抗自训练" class="headerlink" title="对抗自训练"></a>对抗自训练</h2><p>对比学习+拼图学习+相互关系<br>AMDIM 、CPC、Moco 、 Swav 、</p><p>NCE 损失函数 -  SimCLR</p><h2 id="采用训练模型存在的域适应是否匹配问题，是否会造成误匹配"><a href="#采用训练模型存在的域适应是否匹配问题，是否会造成误匹配" class="headerlink" title="采用训练模型存在的域适应是否匹配问题，是否会造成误匹配"></a>采用训练模型存在的域适应是否匹配问题，是否会造成误匹配</h2><h2 id="从零加速训练"><a href="#从零加速训练" class="headerlink" title="从零加速训练"></a>从零加速训练</h2><ul><li>dense connection</li><li>batch normalization<blockquote><p>K. He等随机初始化的实验，证明了ImageNet的随机初始化预训练参数可以加速收敛，但不一定提供正则化或提高最终的检测精度。</p></blockquote></li></ul><h2 id="对抗训练"><a href="#对抗训练" class="headerlink" title="对抗训练"></a>对抗训练</h2><p>In recent two years, GAN has also been applied to object detection, especially for improving the detection of small and occluded object.</p><p>对抗损失 or 域自适应</p><h2 id="弱监督目标检测"><a href="#弱监督目标检测" class="headerlink" title="弱监督目标检测"></a>弱监督目标检测</h2><p>只要求要图片级别的标注。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>the deep CNN is no better than traditional methods for spectral data</p><blockquote><p>O. A. Penatti, K. Nogueira, and J. A. dos Santos, “Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?” in Proceedings of the IEEE conference on computer vision and pattern recognition workshops, 2015, pp. 44–51.</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>阅读总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>目标检测</tag>
      
      <tag>论文总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>无人机-视频检测数据集</title>
    <link href="/2020/09/27/%E6%97%A0%E4%BA%BA%E6%9C%BA-%E8%A7%86%E9%A2%91%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <url>/2020/09/27/%E6%97%A0%E4%BA%BA%E6%9C%BA-%E8%A7%86%E9%A2%91%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
    
    <content type="html"><![CDATA[<h4 id="数据集具有的特点："><a href="#数据集具有的特点：" class="headerlink" title="数据集具有的特点："></a>数据集具有的特点：</h4><ul><li>高密度</li><li>小目标</li><li>相机运动</li><li>实时问题</li></ul><h4 id="数据集-UAVDT"><a href="#数据集-UAVDT" class="headerlink" title="数据集-UAVDT"></a>数据集-UAVDT</h4><h4 id="相关任务："><a href="#相关任务：" class="headerlink" title="相关任务："></a>相关任务：</h4><ul><li>目标检测 DET</li><li>单目标跟踪 SOT</li><li>多目标跟踪 MOT</li></ul><h4 id="MOT"><a href="#MOT" class="headerlink" title="MOT:"></a>MOT:</h4><ul><li>气候条件，例如雾条件</li><li>飞行高度，低飞行高度（10-30m）,中飞行高度（30-70m）,高纬度（70m）</li><li>相机视角，front-view, side-view and bird-view.</li></ul>]]></content>
    
    
    <categories>
      
      <category>阅读总结</category>
      
    </categories>
    
    
    <tags>
      
      <tag>无人机</tag>
      
      <tag>数据集</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>YOLO最新系列的阅读思考</title>
    <link href="/2020/09/24/YOLO%E6%9C%80%E6%96%B0%E7%B3%BB%E5%88%97%E7%9A%84%E9%98%85%E8%AF%BB%E6%80%9D%E8%80%83/"/>
    <url>/2020/09/24/YOLO%E6%9C%80%E6%96%B0%E7%B3%BB%E5%88%97%E7%9A%84%E9%98%85%E8%AF%BB%E6%80%9D%E8%80%83/</url>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://towardsdatascience.com/yolo-v4-or-yolo-v5-or-pp-yolo-dad8e40f7109">https://towardsdatascience.com/yolo-v4-or-yolo-v5-or-pp-yolo-dad8e40f7109</a></p></blockquote><blockquote><p><a href="https://blog.roboflow.com/yolov5-improvements-and-evaluation/">https://blog.roboflow.com/yolov5-improvements-and-evaluation/</a></p></blockquote><p>YOLOv2是注重目标检测速度，但是YOLOv3取得了检测精度和速度的trade off.</p><h2 id="之后跟进的"><a href="#之后跟进的" class="headerlink" title="之后跟进的"></a>之后跟进的</h2><p>YOLOv4 一些架构变化(也是数据增强，但是很奇特了)，提升显著（~12%）<br><code>Photometric Distortion</code> <code>Photometric Distortion</code> <code>Random Erase</code> <code>Cutout</code> <code>Hide and Seek</code> <code>Grid Mask</code> <code>MixUp</code></p><ul><li><code>Mosaic data augmentation</code> :针对小对象问题的解决。</li></ul><p><code>Class label smoothing</code> <code>Self-Adversarial Training (SAT)</code></p><p>YOLOv5 马赛克数据增强（mosaic data augmentation）和框锚自动学习</p><blockquote><p>In the YOLOv3 PyTorch repo, Glenn Jocher introduced the idea of learning anchor boxes based on the distribution of bounding boxes in the custom dataset with K-means and genetic learning algorithms.</p></blockquote><p><strong>这个就是很重要的数据位置和大小预测，YOLOv5做到了自动的学习！！！！！</strong></p><p><code>16位精度浮点</code>需要后续的GPU与框架支持，但是效果显著，尤其是对显存需求大的模型</p><p>网上相关人员给出的评价：yolov4有更大的权重文件，yolov5才仅仅27M，推理速度也达到了140帧，比yolov4快很多！但是准确率减少极少~</p><p>*两模型都基于CSP（其基于DenseNet网络）后续应该彻底研究！！！</p><p>PP-YOLO(基于PaddlePaddle)模型的准确率增加和 tensorRT 的优化也有关系，其准确率和推理速度均优于v4。</p><blockquote><p>Data augmentation in computer vision is key to getting the most out of your dataset, and state of the art research continues to validate this assumption.</p></blockquote><p><a href="https://blog.roboflow.com/yolov4-versus-yolov5/">https://blog.roboflow.com/yolov4-versus-yolov5/</a></p><p>基于这篇较为详细的博客比较，我目前认为v5还是占据着领先</p><p>且yolov5还在继续改进中…</p><p>给出两个链接：</p><p>训练自己的数据集在v5</p><blockquote><p><a href="https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/">https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/</a></p></blockquote><p>训练自己的数据集在v4</p><blockquote><p><a href="https://blog.roboflow.com/training-yolov4-on-a-custom-dataset/">https://blog.roboflow.com/training-yolov4-on-a-custom-dataset/</a></p></blockquote><p>欢迎学习YOLOv5</p><p><a href="https://github.com/ultralytics/yolov5">https://github.com/ultralytics/yolov5</a></p>]]></content>
    
    
    <categories>
      
      <category>目标检测</category>
      
    </categories>
    
    
    <tags>
      
      <tag>调研</tag>
      
      <tag>目标检测</tag>
      
      <tag>算法比较</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>视觉目标跟踪入门</title>
    <link href="/2020/09/24/%E8%A7%86%E8%A7%89%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E5%85%A5%E9%97%A8/"/>
    <url>/2020/09/24/%E8%A7%86%E8%A7%89%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E5%85%A5%E9%97%A8/</url>
    
    <content type="html"><![CDATA[<p>看看这次多久能入门 2020-09-24-11</p><a id="more"></a><p>值得参考的文章：</p><p><a href="https://blog.csdn.net/weixin_38128100/article/details/88343771?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-5.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-5.channel_param#commentBox">https://blog.csdn.net/weixin_38128100/article/details/88343771?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-5.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-5.channel_param#commentBox</a></p><p><a href="https://blog.csdn.net/qq_17783559/article/details/82020940?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-6.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-6.channel_param#commentBox">https://blog.csdn.net/qq_17783559/article/details/82020940?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-6.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-6.channel_param#commentBox</a></p><h4 id="跟踪算法要克服的问题"><a href="#跟踪算法要克服的问题" class="headerlink" title="跟踪算法要克服的问题"></a>跟踪算法要克服的问题</h4><p>形态变化</p><p>尺度变化</p><p>遮挡与消失</p><p>光照强度变化，低分辨率，目标快速移动</p><h3 id="模型分类"><a href="#模型分类" class="headerlink" title="模型分类"></a>模型分类</h3><h4 id="针对目标建模跟踪-（生成模型）"><a href="#针对目标建模跟踪-（生成模型）" class="headerlink" title="针对目标建模跟踪 （生成模型）"></a>针对目标建模跟踪 （生成模型）</h4><p>代表算法有：粒子滤波、光流法、Meanshift算法、Camshift</p><p>利用目标的<code>特征</code>进行跟踪，对后续存在的相似目标进行搜寻，找到类似的目标迭代实现目标的定位</p><p><code>我认为这些是经典算法的意义在于总体的框架构思，这是难以改变的，如果可以突破，那你便是开创了新领域</code></p><p>可以改进的点：</p><ol><li>如果特征采集做的不是很好的话，可能会对后续目标识别影响很大</li><li>加入有效的目标位置预测信息</li></ol><h4 id="考虑目标与背景信息的差异-（鉴别模型）"><a href="#考虑目标与背景信息的差异-（鉴别模型）" class="headerlink" title="考虑目标与背景信息的差异 （鉴别模型）"></a>考虑目标与背景信息的差异 （鉴别模型）</h4><p>很明显，这里就是对框架的革新，属于<code>反向思维</code></p><p>经常受到背景信息的干扰，那不如把这部分信息也有效的利用进来。</p><p>主要两个方向：</p><ol><li>滤波 2014-2015</li><li>深度学习 2015 - </li></ol><h5 id="相关滤波算法"><a href="#相关滤波算法" class="headerlink" title="相关滤波算法"></a>相关滤波算法</h5><ol><li>特征选择 -  单一特征 多个特征</li></ol><p>单特征：比如MOSSE采用的是灰度特征，达到了很高的跟踪速度</p><p>多特征：多个特征进行矢量链接结合，HOG+颜色直方图 or 多个颜色通道的HOG</p><ul><li>深度学习卷积提取特征</li></ul><p>提取特征：例如HOG可以描述表面纹理特征和轮廓形状</p><p>提高速度，采用PCA降维</p><ol start="2"><li>尺度估计 - 简单说就是为了适应目标的大小变化</li></ol><p>我觉得SIFT里面的特征金字塔已经达到很好的水平了</p><p>基于分块的方法，这个思想值得考量：计算各个组件之间的距离，来判断放缩。</p><p>当然，后续提出的不同模型要怎么分块就是更为细致的思考了。</p><p>进一步，利用特征点对来进行尺度放缩的判断也是很妙的想法，减少了很多工作。</p><p><code>我回想我最近看到的这些深度学习的论文，比如YOLOv3,他们对目标的尺寸也是颇有考量的，说明尺寸这里的工作很重要，如果对物体的旋转和尺寸变化很鲁棒，那必是一个不错的算法</code></p><p><code>YOLOv3,算法要针对识别的图像目标，进行计算候选框的最佳尺寸，用的是所有数据的统计结果，这个我认为也是这个算法准确率很高的原因，但是相比RCNN系列，他是在预处理阶段就完成了，莫名奇妙的偷了一点时间出来，哈哈，不过凡是都是有代价的，很明显，YOLO的识别准确率降低了</code></p><p><code>说到这里，我更为抽象化的理解，就是所有的算法在同一个理论框架中都是有局限性的，很多的算法与其说是在改进算法，更不如说是取得了更好的trade off,保持生活的平衡，朋友 or </code></p><blockquote><p>天之道，损有余而补不足。人之道，则不然，损不足以奉有余。孰能有余以奉天下？唯有道者。</p></blockquote><p>》 更为重要的，论文不能说自己最好，绝对有漏洞，或者能达到满足实际的简单需求而已。</p><p><code>回想现在火热的人工智能，常常被吹的神乎其神，内行人都知道，这不过仅仅是开了一条缝的门而已。</code></p><ol><li>分类器的模型 - </li></ol><p>相关研究刚开始是对相关滤波方法的改进。</p><ol><li>对搜索区域进行优化，加入正则化约束</li><li>学习率优化，自适应函数</li><li>结合相关滤波的循环矩阵更新过程，多个峰值停止更新，增加判断机制</li><li>在线检测机制，设置阈值来判断更新过程的可靠程度</li></ol><h3 id="深度学习下的跟踪方法"><a href="#深度学习下的跟踪方法" class="headerlink" title="深度学习下的跟踪方法"></a>深度学习下的跟踪方法</h3><p>可以说这个是不同于上述的算法框架的。</p><p>端到端，以大数据来引领算法是这个领域的主要特征。</p><p>没有数据，何以谈深度学习，庆幸的是这个时代最不缺的就是数据，未发掘的数据巨大！</p><p><code>进入正题，基于CNN对特征进行提取。</code></p><ol><li>对相关滤波方法进行改进，利用不同层卷积提取的特征信息来训练相关滤波器。</li><li>DeepSRDCF建立单层神经网络来减少深度特征的计算量</li><li>C-COT 连续卷积滤波器，连续空间域差值转换，输入不同分辨率特征</li><li>ECO进一步改进，去除贡献小的滤波器，降低维度；6帧更新一次，可减少计算和减轻目标遮挡带来的影响（躲避困难的胜利？）</li><li>SiamFC 孪生网络思想</li></ol><blockquote><p>Bertinetto L, Valmadre J, Henriques J F, Vedaldi A, Torr P<br>H S. Fully-convolutional siamese networks for object track-<br>ing. In: Proceedings of Computer Vision. Lecture Notes<br>in Computer Science, vol. 9914. Amsterdam, Netherlands: Springer, 2016. 850-865</p></blockquote><ol start="6"><li>DLT 使用判断相似性来更新阈值的方法来进行网络更新。但是要是出现目标和目标混乱情况时会出现一直更新，实时性变差</li></ol><blockquote><p>Wang N Y, Yeung D Y. Learning a deep compact image<br>representation for visual tracking. In: Proceedings of the<br>26th International Conference on Neural Information Pro-<br>cessing Systems. Lake Tahoe, Nevada: Curran Associates<br>Inc., 2013. 809-817</p></blockquote><ol start="7"><li>将高斯函数引入到卷积神将网络中训练中,来提高深度特征提取的精度？</li></ol><blockquote><p>Ding J W, Huang Y Z, Liu W, Huang K Q. Severely<br>blurred object tracking by learning deep image representa-<br>tions. IEEE Transactions on Circuits and Systems for Video<br>Technology, 2016, 26(2): 319-331</p></blockquote><ol start="8"><li>对不同的网络层次合理采用来获取不同层次的特征，有效的过滤干扰噪声 <code>让我想到了Unet++，思想框架如出一致。</code></li></ol><blockquote><p>Dai L, Zhu Y S, Luo G B, He C. A low-complexity visual<br>tracking approach with single hidden layer neural networks.<br>In: Proceedings of the 13th International Conference on<br>Control Automation Robotics and Vision. Singapore: IEEE, 2014. 810-814</p></blockquote><ol start="9"><li>神经网络树？？？77</li></ol><blockquote><p>Nam H, Baek M, Han B. Modeling and propagating CNNs<br>in a tree structure for visual tracking. In: Proceedings of<br>the 2016 IEEE Conference on Computer Vision and Pat-<br>tern Recognition. Las Vegas, USA: IEEE, 2016. 2137-2155</p></blockquote><ol start="10"><li>正负样本不平衡 - 生成对抗网络丰富样本系列，数据增强都可以考虑这个模型。</li><li>高阶敏感损失？？？ </li></ol><blockquote><p>Song Y B, Ma C, Wu X H, Gong L J, Bao L C, Zuo W M, et<br>al. Visual tracking via adversarial learning. In: Proceedings<br>of the 2018 IEEE Conference on Computer Vision and Pat-<br>tern Recognition. Salt Lake City, Utah, USA: IEEE, 2018. 1084-1093</p></blockquote><p>截止2018，实验效果不错的是ECO</p><blockquote><p>一个很神的算法框架：Boost思想，对于提升算法各项能力都很重要。</p></blockquote><h3 id="仅仅是一些无关紧要的"><a href="#仅仅是一些无关紧要的" class="headerlink" title="仅仅是一些无关紧要的"></a>仅仅是一些无关紧要的</h3><p>学到了一个单词，之前用过，但是用错了（原理图，Schematic diagram）</p><p>数据集：<br> OTB-2013</p><p>留待思考：</p><p>深度学习类算法跟踪速度过慢都多都在0.1-5 帧每秒之间,不能达到实时跟踪; 而相关滤波类算法跟踪速度可以达到百帧每秒, 可以对目标实时跟踪.</p><p>问题是出在模型更新机制？</p><p>各方法优势的结合，或者更好的范式吗？</p>]]></content>
    
    
    <categories>
      
      <category>目标跟踪</category>
      
    </categories>
    
    
    <tags>
      
      <tag>调研</tag>
      
      <tag>跟踪</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>百度首支纪录片《二十度》</title>
    <link href="/2020/09/24/%E7%99%BE%E5%BA%A6%E9%A6%96%E6%94%AF%E7%BA%AA%E5%BD%95%E7%89%87%E3%80%8A%E4%BA%8C%E5%8D%81%E5%BA%A6%E3%80%8B/"/>
    <url>/2020/09/24/%E7%99%BE%E5%BA%A6%E9%A6%96%E6%94%AF%E7%BA%AA%E5%BD%95%E7%89%87%E3%80%8A%E4%BA%8C%E5%8D%81%E5%BA%A6%E3%80%8B/</url>
    
    <content type="html"><![CDATA[<ul><li><p>现在的算法就是为了取悦用户，试图使得用户在应用上消耗的时间越多。</p></li><li><p>李彦宏早上6点半起床，晚上12点睡觉。</p></li><li><p>方法论？</p></li><li><p>学习环境问题，信息资源获取不平等</p></li><li><p>留学，接触到互联网技术，结合市场时机进行创业</p></li><li><p>为了避免产权问题，不亲手写论文，4个月开发出来搜索技术，很好（天使轮投资）</p></li><li><p>有些东西很重要，如，用户体验，如响应速度！！！</p></li><li><p>注重锻炼！！!</p></li><li><p>媒体公开工作 - 当时做的工作很出色～</p></li><li><p>大数据算法思维</p></li><li><p>和最优秀的团队PK，小公司更加可以专注与某件事</p></li><li><p>对网页进行搜录，全面覆盖</p></li><li><p>适合中国人的使用习惯</p></li><li><p>牛卡计划，摆脱被恶意收购</p></li><li><p>为国争光</p></li><li><p>互联网生态变化，网络资源萎缩</p></li><li><p>碎片化明显，你最多做的动作就是收藏！！！</p></li><li><p>对移动端进行转型，时代潮流</p></li><li><p>降低知识的门槛</p></li><li><p>承受住流量高峰，证明了自己的实力</p></li><li><p>信仰技术，技术可以改变世界，掌握核心技术</p></li><li><p>没有明确收益进行投入很不容易</p></li><li><p>黑客马拉松</p></li><li><p>对未来进行的布局和展望</p></li><li><p>长期在基础技术领域进行投入 <code>硬实力</code></p></li><li><p>信息世界，信息永生，虚拟重建</p></li><li><p>全栈式 - 创新研究</p></li><li><p>机器 与 强人工智能 ， 只是梦？</p></li><li><p>深度学习平台</p></li><li><p>开源 是为了展宽道路，变得更加丰富</p></li><li><p>专利技术 5712</p></li><li><p>自动驾驶 遥遥领先， 15%的研发投入</p></li><li><p>不惧亏损，敢于投入</p></li><li><p>积攒能量，等待～</p></li><li><p>百度会再次崛起，依靠硬实力</p></li><li><p>涵养，理性，尊重</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>百度</tag>
      
      <tag>公司发展与人生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ubuntu开机运行程序</title>
    <link href="/2020/09/24/unbuntu%E5%BC%80%E6%9C%BA%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F/"/>
    <url>/2020/09/24/unbuntu%E5%BC%80%E6%9C%BA%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F/</url>
    
    <content type="html"><![CDATA[<p>作为开机启动项的记录，用来对比进程。</p><pre><code class="hljs bash"> PID TTY          TIME CMD1290 tty1     00:00:02 Xorg1317 tty1     00:00:00 gnome-session-b1349 tty1     00:00:02 gnome-shell2065 tty1     00:00:00 ibus-daemon2068 tty1     00:00:00 ibus-dconf2071 tty1     00:00:00 ibus-x112147 tty1     00:00:00 gsd-xsettings2150 tty1     00:00:00 gsd-a11y-settin2151 tty1     00:00:00 gsd-clipboard2153 tty1     00:00:00 gsd-color2154 tty1     00:00:00 gsd-datetime2157 tty1     00:00:00 gsd-housekeepin2160 tty1     00:00:00 gsd-keyboard2163 tty1     00:00:00 gsd-media-keys2167 tty1     00:00:00 gsd-mouse2169 tty1     00:00:00 gsd-power2177 tty1     00:00:00 gsd-print-notif2179 tty1     00:00:00 gsd-rfkill2181 tty1     00:00:00 gsd-screensaver2196 tty1     00:00:00 gsd-sharing2197 tty1     00:00:00 gsd-smartcard2201 tty1     00:00:00 gsd-sound2210 tty1     00:00:00 gsd-wacom2275 tty1     00:00:00 ibus-engine-sim2382 tty2     00:00:12 Xorg2394 tty2     00:00:00 gnome-session-b2522 tty2     00:00:28 gnome-shell2563 tty2     00:00:00 ibus-daemon2567 tty2     00:00:00 ibus-dconf2569 tty2     00:00:00 ibus-x112647 tty2     00:00:00 gsd-power2648 tty2     00:00:00 gsd-print-notif2651 tty2     00:00:00 gsd-rfkill2653 tty2     00:00:00 gsd-screensaver2654 tty2     00:00:00 gsd-sharing2657 tty2     00:00:00 gsd-xsettings2662 tty2     00:00:00 gsd-wacom2664 tty2     00:00:00 gsd-smartcard2665 tty2     00:00:00 gsd-sound2677 tty2     00:00:00 gsd-a11y-settin2680 tty2     00:00:00 gsd-clipboard2685 tty2     00:00:00 gsd-color2690 tty2     00:00:00 gsd-datetime2695 tty2     00:00:00 gsd-housekeepin2696 tty2     00:00:00 gsd-keyboard2699 tty2     00:00:00 gsd-media-keys2701 tty2     00:00:00 gsd-mouse2727 tty2     00:00:00 gsd-printer2764 tty2     00:00:01 nautilus-deskto2766 tty2     00:00:00 indicator-sysmo2767 tty2     00:00:00 gsd-disk-utilit2831 tty2     00:00:00 python2847 tty2     00:00:00 ibus-engine-lib3875 tty2     00:00:00 update-notifier3878 tty2     00:00:01 gnome-software5174 pts/0    00:00:00 sudo5178 pts/0    00:00:00 ps</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>linux</tag>
      
      <tag>ubuntu</tag>
      
      <tag>ps</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>让我看看怎么写博客（语法类）</title>
    <link href="/2020/09/23/%E8%AE%A9%E6%88%91%E7%9C%8B%E7%9C%8B%E6%80%8E%E4%B9%88%E5%86%99%E5%8D%9A%E5%AE%A2%EF%BC%88%E8%AF%AD%E6%B3%95%E7%B1%BB%EF%BC%89/"/>
    <url>/2020/09/23/%E8%AE%A9%E6%88%91%E7%9C%8B%E7%9C%8B%E6%80%8E%E4%B9%88%E5%86%99%E5%8D%9A%E5%AE%A2%EF%BC%88%E8%AF%AD%E6%B3%95%E7%B1%BB%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<pre><code class="hljs markdown">hexo clean &amp;&amp; hexo g # 清理缓存生成文件hexo d # 部署文档hexo s # 本地测试</code></pre><p><code>列表：</code></p><ol><li>健康</li><li>幸福</li><li>事业</li></ol><ul><li>点 <code>-</code></li></ul><pre><code class="hljs markdown">&#123;% note primary %&#125;这里可以写文字 或者 <span class="hljs-code">`markdown`</span>&#123;% endnote %&#125;&#123;% note warning %&#125;这里可以写文字 或者 <span class="hljs-code">`markdown`</span>&#123;% endnote %&#125;&#123;% note danger %&#125;这里可以写文字 或者 <span class="hljs-code">`markdown`</span>&#123;% endnote %&#125;</code></pre><div class="note note-primary">            <p>这里可以写文字 或者 <code>markdown</code></p>          </div><span class="label label-primary">行内标签</span><pre><code class="hljs markdown">&#123;% label primary @行内标签 %&#125;</code></pre><div>            <input type="checkbox" disabled checked="checked">主要是解决一些 Renderer 不支持勾选          </div><pre><code class="hljs markdown">&#123;% cb 主要是解决一些 Renderer 不支持勾选, true %&#125;</code></pre><a class="btn" href="javascript:;"  target="_blank">支持链接</a> <pre><code class="hljs markdown">&#123;% btn javascript:;, 支持链接 %&#125;</code></pre><p>还要常去翻阅Markdown笔记！<section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>脚注演示<br><pre><code class="hljs markdown">[<span class="hljs-symbol">^1</span>]: <span class="hljs-link">脚注演示</span></code></pre><br><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section></p>]]></content>
    
    
    <categories>
      
      <category>Markdown</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MD</tag>
      
      <tag>Fluid</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>为什么要建博客</title>
    <link href="/2020/09/23/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%BB%BA%E5%8D%9A%E5%AE%A2/"/>
    <url>/2020/09/23/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%BB%BA%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<h1 id="建立博客的初衷"><a href="#建立博客的初衷" class="headerlink" title="建立博客的初衷"></a>建立博客的初衷</h1><p>以前建立博客可能只是为了好玩，为了有个对网络的好奇。 但是之后的不断建站也是看到了大佬们的经验分享的必要，不一定要写的多好，但是可以记录自己的一些问题，就很好了。</p><a id="more"></a><p>这算是我第三次折腾这个了吧，希望以后就不要在折腾了，也别放弃了，简简单单做个记录，此次网站已经足够了。</p><p>第一次（2018）我作为一个小白，发现建立一个博客也很简单，我就着手尝试进行了轰轰烈烈的建站活动，然后也找了官方的例程和其他伙伴的博客进行学习、模仿。作为一个体验还可以的，但是实际做出来的效果差强人意，虽然也受到了陌生好友的鼓励，但是自己实在受不了了，后面随着换电脑就放弃了。</p><p>第二次（2020.2）第一次的经历确实不太好，那时各种插件还不是那么人性化，给我留下不好的印象。先说第二次，受到Markdeep的启发，我觉得这样做或许很容易，结果还是不是那么容易，但是网站几乎达到了自己的要求，也算可以，还做了一个总结发在了知乎上面。</p><p>第三次（2020.9）这时我又换了电脑，也经过一个月的尝试，到了linux环境下进行工作学习，这时候我又萌发了写记录文章的念头。每天查的问题和总结的文献，解决的方案很多很多，昨天做的工作今天要用到，就已经忘光了。这个状况让我很是不安，如果很多东西还要不断的重复的话，那会造成我的时间很大的浪费的，我的危机感突然升起。</p><p>建站的初衷（又回来了），如果可以将自己的知识尽可能的记录下来，省下时间可以更多的提升自己。</p><p>记录下来这个心路历程，与大家共勉！！！</p><p>PS:吐槽也是博客的一大乐趣。:-D</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>感悟</tag>
      
      <tag>博客</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
